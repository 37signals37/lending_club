{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8675c944",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe14965",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d313fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: imports, settings, and helpers\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from scipy.stats import fisher_exact\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e830fb2",
   "metadata": {},
   "source": [
    "## Notebook Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a240f8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "pd.set_option('display.max_columns', 120)\n",
    "pd.set_option('display.width', 160)\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = 'lending_club_dataset.csv'\n",
    "SCHEMA_VALIDATION_PATH = 'schema_validation_sheet.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03670c7f",
   "metadata": {},
   "source": [
    "## Loading the Dataset and Schema Validation Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12919f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: safe read CSV with basic dtype hints\n",
    "def read_csv_safely(path: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Dataset not found at {path}. Please ensure the CSV is in the workspace root.\")\n",
    "    # Low-memory to reduce dtype inference churn\n",
    "    return pd.read_csv(path, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72d1edb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with shape: (10000, 28)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "try:\n",
    "    df_raw = read_csv_safely(DATA_PATH)\n",
    "    print(\"Loaded dataset with shape:\", df_raw.shape)\n",
    "except Exception as e:\n",
    "    print(\"Failed to load dataset:\", e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "52c23ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with shape: (28, 10)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "try:\n",
    "    df_schema_validation = read_csv_safely(SCHEMA_VALIDATION_PATH)\n",
    "    print(\"Loaded dataset with shape:\", df_schema_validation.shape)\n",
    "except Exception as e:\n",
    "    print(\"Failed to load dataset:\", e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bf460c",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "635f0257",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42 # Random seed for reproducibility\n",
    "\n",
    "# Split Ratios\n",
    "TRAIN_SIZE = 0.8  # Proportion of data for training set\n",
    "VAL_SIZE = 0.1  # Proportion of data for validation set\n",
    "TEST_SIZE = 0.1  # Proportion of data for test set\n",
    "\n",
    "# Columns\n",
    "TARGET_COL = 'is_bad'  # Target column for classification\n",
    "NUMERICAL_COLS = [\"annual_inc\",\"delinq_2yrs\"]\n",
    "CAT_COLS = [\"home_ownership\", \"verification_status\", \"pymnt_plan\", \"purpose_cat\", \"zip_code\", \"addr_state\", ]\n",
    "# CONT_COLS = [\"annual_inc\", \"debt_to_income\", \"\"]\n",
    "# TEXT_COLS = [\"Notes\", \"purpose\", ]\n",
    "# RAT_COLS = [\"delinq_2yrs\"]\n",
    "# DISCRETE_COLS = [\"open_acc\", \"pub_rec\", \"total_acc\", \"acc_now_delinq\", \"tot_coll_amt\", \"tot_cur_bal\", \"total_rev_hi_lim\"]\n",
    "# DATE_COLS = [\"earliest_cr_line\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0104b7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that MISSING_VALUE_FILL is not in the dataset\n",
    "if MISSING_VALUE_FILL in df_raw.values:\n",
    "    raise ValueError(f\"MISSING_VALUE_FILL value {MISSING_VALUE_FILL} already exists in the dataset. Choose a different placeholder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d0a739f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = []\n",
    "splits = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3562a43",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcecf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column description\n",
    "def describe_column(df: pd.DataFrame, col: str):\n",
    "    print(f\"Missing Values: {df[col].isnull().sum()}\")\n",
    "    print(f\"Unique Values: {df[col].nunique()}\")\n",
    "\n",
    "    try:\n",
    "        print(f\"Min Value: {df[col].min()}\")\n",
    "        print(f\"Max Value: {df[col].max()}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print(df[col].value_counts(dropna=False).head(10))\n",
    "    print()\n",
    "    print(df[col].describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a0d2428d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'expected_dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Local_Code_Projects/lending_club/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'expected_dtype'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[115]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     50\u001b[39m             issues[col] = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mColumn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m has dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m issues\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[43mcheck_against_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_raw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_schema_validation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[115]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mcheck_against_schema\u001b[39m\u001b[34m(df, schema_df)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m schema_df.iterrows():\n\u001b[32m     10\u001b[39m     col = row[\u001b[33m'\u001b[39m\u001b[33mcolumn\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     expected_dtype = \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mexpected_dtype\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     12\u001b[39m     unique_min = row[\u001b[33m'\u001b[39m\u001b[33munique_min\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     13\u001b[39m     unique_max = row[\u001b[33m'\u001b[39m\u001b[33munique_max\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Local_Code_Projects/lending_club/.venv/lib/python3.13/site-packages/pandas/core/series.py:1130\u001b[39m, in \u001b[36mSeries.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[key]\n\u001b[32m   1129\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[32m-> \u001b[39m\u001b[32m1130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[32m   1133\u001b[39m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[32m   1134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Local_Code_Projects/lending_club/.venv/lib/python3.13/site-packages/pandas/core/series.py:1246\u001b[39m, in \u001b[36mSeries._get_value\u001b[39m\u001b[34m(self, label, takeable)\u001b[39m\n\u001b[32m   1243\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[label]\n\u001b[32m   1245\u001b[39m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1246\u001b[39m loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[32m   1249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[loc]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Local_Code_Projects/lending_club/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'expected_dtype'"
     ]
    }
   ],
   "source": [
    "def check_against_schema(df: pd.DataFrame, schema_df: pd.DataFrame):\n",
    "    issues = {}\n",
    "\n",
    "    cols_missing_from_schema = set(df.columns) - set(schema_df.columns)\n",
    "    issues['missing_from_schema'] = list(cols_missing_from_schema)\n",
    "    cols_missing_from_dataframe = set(schema_df.columns) - set(df.columns)\n",
    "    issues['missing_from_dataframe'] = list(cols_missing_from_dataframe)\n",
    "\n",
    "    for _, row in schema_df.iterrows():\n",
    "        col = row['column']\n",
    "        expected_dtype = row['expected_dtype']\n",
    "        unique_min = row['unique_min']\n",
    "        unique_max = row['unique_max']\n",
    "        value_min = row['value_min']\n",
    "        value_max = row['value_max']\n",
    "        \n",
    "        if col in df.columns:            \n",
    "            col_issues = []\n",
    "            if expected_dtype:\n",
    "                actual_dtype = str(df[col].dtype)\n",
    "                if expected_dtype not in actual_dtype:\n",
    "                    col_issues.append(f\"Expected dtype {expected_dtype}, found {actual_dtype}.\")\n",
    "            if pd.notnull(unique_min):\n",
    "                unique_count = df[col].nunique()\n",
    "                if unique_count < unique_min:\n",
    "                    col_issues.append(f\"Unique count {unique_count} less than minimum {unique_min}.\")\n",
    "            if pd.notnull(unique_max):\n",
    "                unique_count = df[col].nunique()\n",
    "                if unique_count > unique_max:\n",
    "                    col_issues.append(f\"Unique count {unique_count} greater than maximum {unique_max}.\")\n",
    "            if pd.notnull(value_min):\n",
    "                try:\n",
    "                    min_value = df[col].min()\n",
    "                    if min_value < value_min:\n",
    "                        col_issues.append(f\"Min value {min_value} less than minimum {value_min}.\")\n",
    "                except:\n",
    "                    pass\n",
    "            if pd.notnull(value_max):\n",
    "                try:\n",
    "                    max_value = df[col].max()\n",
    "                    if max_value > value_max:\n",
    "                        col_issues.append(f\"Max value {max_value} greater than maximum {value_max}.\")\n",
    "                except:\n",
    "                    pass\n",
    "            if col_issues:\n",
    "                issues[col] = col_issues\n",
    "        \n",
    "        actual_dtype = str(df[col].dtype)\n",
    "        if expected_dtype not in actual_dtype:\n",
    "            issues[col] = f\"Column {col} has dtype {actual_dtype}, expected {expected_dtype}.\"\n",
    "\n",
    "    return issues\n",
    "\n",
    "check_against_schema(df_raw, df_schema_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5d41f0",
   "metadata": {},
   "source": [
    "## Statistical Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e5e4af",
   "metadata": {},
   "source": [
    "### Fisher's Exact Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c5151ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisher_exact_test(df: pd.DataFrame, feature_col: str, positive_feature: int, target_col: str):\n",
    "    contingency_table = pd.crosstab(df[feature_col] == positive_feature, df[target_col])\n",
    "\n",
    "    print(\"Contingency table:\\n\", contingency_table)\n",
    "\n",
    "    # Fisher's exact test\n",
    "    odds_ratio, p_value = fisher_exact(contingency_table)\n",
    "\n",
    "    print(f\"Fisher's Exact Test odds ratio: {odds_ratio:.4f}\")\n",
    "    print(f\"p-value: {p_value:.4g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbf28aa",
   "metadata": {},
   "source": [
    "# Pre-split EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d66f1aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_near_constant_features(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    dominant_thresh: float = 0.98,   # flag if top value covers ≥ this fraction of non-null values\n",
    "    max_unique_for_flag: int | None = None,  # optionally also require unique values ≤ this number\n",
    "    min_non_null: int = 5,           # skip columns with too few non-nulls\n",
    "    treat_bool_as_categorical: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze columns for 'near-constant' behavior and return a summary DataFrame.\n",
    "\n",
    "    Columns are flagged when the dominant (most frequent) value's share among non-null values\n",
    "    is ≥ dominant_thresh. Optionally (if max_unique_for_flag is set), we also require that the\n",
    "    number of unique non-null values ≤ max_unique_for_flag.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input data.\n",
    "    dominant_thresh : float, default 0.98\n",
    "        Threshold for dominant value proportion to flag near-constant.\n",
    "    max_unique_for_flag : int or None, default None\n",
    "        If set, near-constant flag additionally requires unique_count ≤ this.\n",
    "        (E.g., set to 2 to flag only nearly-all-0/1 columns.)\n",
    "    min_non_null : int, default 5\n",
    "        Skip columns with fewer than this many non-null observations.\n",
    "    treat_bool_as_categorical : bool, default True\n",
    "        If True, boolean columns are summarized as categoricals.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Columns:\n",
    "        - column\n",
    "        - dtype\n",
    "        - non_null_count\n",
    "        - missing_rate\n",
    "        - unique_count\n",
    "        - dominant_value\n",
    "        - dominant_count\n",
    "        - dominant_share\n",
    "        - minority_count\n",
    "        - variance (numeric only; else NaN)\n",
    "        - std (numeric only; else NaN)\n",
    "        - min_value (numeric only; else NaN)\n",
    "        - max_value (numeric only; else NaN)\n",
    "        - entropy_bits (Shannon entropy base-2 on value distribution)\n",
    "        - is_binary_like (unique_count == 2)\n",
    "        - near_constant_flag (boolean)\n",
    "        - top_5_values (string formatted \"value:count; …\")\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        s = df[col]\n",
    "        non_null = s.dropna()\n",
    "        non_null_count = int(non_null.shape[0])\n",
    "        missing_rate = (1.0 - (non_null_count / max(1, s.shape[0])))*100\n",
    "\n",
    "        # Handle dtype classification early so branches can use it\n",
    "        is_bool = pd.api.types.is_bool_dtype(s)\n",
    "        is_numeric = pd.api.types.is_numeric_dtype(s) and not (is_bool and treat_bool_as_categorical)\n",
    "\n",
    "        # Precompute min/max for numeric columns\n",
    "        if is_numeric and non_null_count > 0:\n",
    "            try:\n",
    "                min_value = float(non_null.astype(float).min())\n",
    "                max_value = float(non_null.astype(float).max())\n",
    "            except Exception:\n",
    "                # Fallback in case of mixed numeric types that fail astype(float)\n",
    "                min_value = float(pd.to_numeric(non_null, errors='coerce').min())\n",
    "                max_value = float(pd.to_numeric(non_null, errors='coerce').max())\n",
    "        else:\n",
    "            min_value = np.nan\n",
    "            max_value = np.nan\n",
    "\n",
    "        # # Top-5 most common values (include NaN label consistent with helper)\n",
    "        # try:\n",
    "        #     top5_vals = top5_summary(s)\n",
    "        # except Exception:\n",
    "        #     # If helper not available for any reason, compute a simple fallback\n",
    "        vc_tmp = s.value_counts(dropna=False).head(5)\n",
    "        def _fmt(v):\n",
    "            if pd.isna(v):\n",
    "                return 'NaN'\n",
    "            txt = str(v)\n",
    "            return (txt[:60] + '…') if len(txt) > 60 else txt\n",
    "        top5_vals = \"; \".join([f\"{_fmt(idx)}:{int(cnt)}\" for idx, cnt in vc_tmp.items()])\n",
    "\n",
    "        if non_null_count < min_non_null:\n",
    "            # Not enough data to assess; still record info\n",
    "            summaries.append({\n",
    "                \"column\": col,\n",
    "                \"dtype\": s.dtype.name,\n",
    "                \"non_null_count\": non_null_count,\n",
    "                \"missing_rate\": missing_rate,\n",
    "                \"unique_count\": non_null.nunique(dropna=True),\n",
    "                \"min_value\": min_value,\n",
    "                \"max_value\": max_value,\n",
    "                \"dominant_value\": np.nan,\n",
    "                \"dominant_count\": 0,\n",
    "                \"dominant_share\": np.nan,\n",
    "                \"minority_count\": 0,\n",
    "                \"variance\": np.nan,\n",
    "                \"std\": np.nan,\n",
    "                \"entropy_bits\": np.nan,\n",
    "                \"is_binary_like\": False,\n",
    "                \"near_constant_flag\": False,\n",
    "                \"top_5_values\": top5_vals,\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Value counts for frequencies\n",
    "        vc = non_null.value_counts(dropna=False)\n",
    "        dominant_value = vc.index[0]\n",
    "        dominant_count = int(vc.iloc[0])\n",
    "        unique_count = int(vc.shape[0])\n",
    "        dominant_share = dominant_count / non_null_count\n",
    "        minority_count = non_null_count - dominant_count\n",
    "\n",
    "        # Entropy (base-2)\n",
    "        probs = (vc / non_null_count).to_numpy()\n",
    "        entropy_bits = float(-np.sum(probs * np.log2(probs))) if unique_count > 1 else 0.0\n",
    "\n",
    "        # Numeric variance/std if applicable\n",
    "        if is_numeric:\n",
    "            variance = float(non_null.astype(float).var(ddof=1)) if non_null_count > 1 else 0.0\n",
    "            std = float(np.sqrt(variance))\n",
    "        else:\n",
    "            variance = np.nan\n",
    "            std = np.nan\n",
    "\n",
    "        # Binary-like flag\n",
    "        is_binary_like = (unique_count == 2)\n",
    "\n",
    "        # Near-constant logic\n",
    "        meets_share = dominant_share >= dominant_thresh\n",
    "        meets_unique = True if max_unique_for_flag is None else (unique_count <= max_unique_for_flag)\n",
    "        near_constant_flag = bool(meets_share and meets_unique)\n",
    "\n",
    "        summaries.append({\n",
    "            \"column\": col,\n",
    "            \"dtype\": s.dtype.name,\n",
    "            \"non_null_count\": non_null_count,\n",
    "            \"missing_rate\": missing_rate,\n",
    "            \"unique_count\": unique_count,\n",
    "            \"min_value\": min_value,\n",
    "            \"max_value\": max_value,\n",
    "            \"dominant_value\": dominant_value,\n",
    "            \"dominant_count\": dominant_count,\n",
    "            \"dominant_share\": dominant_share,\n",
    "            \"minority_count\": minority_count,\n",
    "            \"variance\": variance,\n",
    "            \"std\": std,\n",
    "            \"entropy_bits\": entropy_bits,\n",
    "            \"is_binary_like\": is_binary_like,\n",
    "            \"near_constant_flag\": near_constant_flag,\n",
    "            \"top_5_values\": top5_vals,\n",
    "        })\n",
    "\n",
    "    out = pd.DataFrame(summaries)\n",
    "    # Order: flags first, then by dominant_share desc, then low entropy\n",
    "    # out = out.sort_values(\n",
    "    #     by=[\"near_constant_flag\", \"dominant_share\", \"entropy_bits\"],\n",
    "    #     ascending=[False, False, True],\n",
    "    #     kind=\"mergesort\"\n",
    "    # ).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "# Recompute summary with new columns\n",
    "df_summary = summarize_near_constant_features(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e4dcd14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>dtype</th>\n",
       "      <th>non_null_count</th>\n",
       "      <th>missing_rate</th>\n",
       "      <th>unique_count</th>\n",
       "      <th>min_value</th>\n",
       "      <th>max_value</th>\n",
       "      <th>dominant_value</th>\n",
       "      <th>dominant_count</th>\n",
       "      <th>dominant_share</th>\n",
       "      <th>minority_count</th>\n",
       "      <th>variance</th>\n",
       "      <th>std</th>\n",
       "      <th>entropy_bits</th>\n",
       "      <th>is_binary_like</th>\n",
       "      <th>near_constant_flag</th>\n",
       "      <th>top_5_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>emp_length</td>\n",
       "      <td>object</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>2160</td>\n",
       "      <td>0.216</td>\n",
       "      <td>7840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.137702</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>10:2160; 1:2083; 2:1183; 3:1010; 4:889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       column   dtype  non_null_count  missing_rate  unique_count  min_value  max_value dominant_value  dominant_count  dominant_share  minority_count  \\\n",
       "3  emp_length  object           10000           0.0            14        NaN        NaN             10            2160           0.216            7840   \n",
       "\n",
       "   variance  std  entropy_bits  is_binary_like  near_constant_flag                            top_5_values  \n",
       "3       NaN  NaN      3.137702           False               False  10:2160; 1:2083; 2:1183; 3:1010; 4:889  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary[3:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3f516f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols.append(\"pymnt_plan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e337ab",
   "metadata": {},
   "source": [
    "# Pre-split Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15d01e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_presplit_trans = df_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9f8a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "df_presplit_trans[\"initial_list_status\"] = encoder.fit_transform(df_raw[\"initial_list_status\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478085ec",
   "metadata": {},
   "source": [
    "## collections_12_mths_ex_med\n",
    "Type of missingness is unknown at this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b67765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # collections_12_mths_ex_med consists of 0s and missing values; will convert missing values to -1 and examine further after post-split\n",
    "# print(df_raw[\"collections_12_mths_ex_med\"].value_counts())\n",
    "# print(df_raw[\"collections_12_mths_ex_med\"].isna().sum())\n",
    "\n",
    "# #convert \"NA\" values to -1\n",
    "# df_raw[\"collections_12_mths_ex_med\"] = df_raw[\"collections_12_mths_ex_med\"].fillna(-1)\n",
    "# print()\n",
    "# print(df_raw[\"collections_12_mths_ex_med\"].value_counts())\n",
    "# print(df_raw[\"collections_12_mths_ex_med\"].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fc6447",
   "metadata": {},
   "source": [
    "# Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2edcd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 8000\n",
      "Validation size: 1000\n",
      "Test size: 1000\n",
      "\n",
      "Class distribution in train:\n",
      "is_bad\n",
      "0    0.8705\n",
      "1    0.1295\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class distribution in validation:\n",
      "is_bad\n",
      "0    0.871\n",
      "1    0.129\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class distribution in test:\n",
      "is_bad\n",
      "0    0.87\n",
      "1    0.13\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "target_col = 'is_bad'\n",
    "X = df_presplit_trans.drop(columns=[target_col])\n",
    "y = df_presplit_trans[target_col]\n",
    "\n",
    "# First: split off the test set (10%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.10,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Next: split the remaining 90% into train (80%) and validation (10%)\n",
    "# Since test already took 10%, we want 10/90 = ~0.1111 of the remainder as validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=1/9,   # ≈ 0.1111\n",
    "    stratify=y_temp,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "splits = [X_train, X_val, X_test, y_train, y_val, y_test]    \n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Validation size:\", len(X_val))\n",
    "print(\"Test size:\", len(X_test))\n",
    "\n",
    "# Check stratification worked (class balance preserved)\n",
    "print(\"\\nClass distribution in train:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nClass distribution in validation:\")\n",
    "print(y_val.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nClass distribution in test:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89a2621",
   "metadata": {},
   "source": [
    "# Post-split EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4a8a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_presplit_trans[df_presplit_trans['Id'].isin(X_train['Id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfce2bb0",
   "metadata": {},
   "source": [
    "## collections_12_mths_ex_med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "db525053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contingency table:\n",
      " is_bad                         0     1\n",
      "collections_12_mths_ex_med            \n",
      "False                       6936  1035\n",
      "True                          28     1\n",
      "Fisher's Exact Test odds ratio: 0.2393\n",
      "p-value: 0.1669\n"
     ]
    }
   ],
   "source": [
    "# collections_12_mths_ex_med has 29 missing values and by itself likely contains little information for prediction; confirming with Fisher's exact test\n",
    "df_analysis[\"collections_12_mths_ex_med\"] = df_analysis[\"collections_12_mths_ex_med\"].fillna(-1)\n",
    "df_analysis[\"collections_12_mths_ex_med\"].value_counts()\n",
    "fisher_exact_test(df_analysis, 'collections_12_mths_ex_med', -1, TARGET_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "60ce8a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols.append('collections_12_mths_ex_med')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293256ee",
   "metadata": {},
   "source": [
    "## initial_list_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "499f4d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in initial_list_status: 0\n",
      "initial_list_status\n",
      "0    7986\n",
      "1      14\n",
      "Name: count, dtype: int64\n",
      "Contingency table:\n",
      " is_bad                  0     1\n",
      "initial_list_status            \n",
      "False                6950  1036\n",
      "True                   14     0\n",
      "Fisher's Exact Test odds ratio: 0.0000\n",
      "p-value: 0.2399\n"
     ]
    }
   ],
   "source": [
    "# initial_list_status\n",
    "col = 'initial_list_status'\n",
    "print(f\"Missing values in {col}: {df_analysis[col].isna().sum()}\")\n",
    "print(df_analysis[col].value_counts())\n",
    "\n",
    "fisher_exact_test(df_analysis, col, 1, TARGET_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1321dcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols.append('initial_list_status')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa6111a",
   "metadata": {},
   "source": [
    "# Post-Split Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "15707fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['collections_12_mths_ex_med',\n",
       " 'collections_12_mths_ex_med',\n",
       " 'pymnt_plan',\n",
       " 'initial_list_status']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "25ed2f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Column 'collections_12_mths_ex_med' not found in X_train\n",
      "Warning: Column 'collections_12_mths_ex_med' not found in X_val\n",
      "Warning: Column 'collections_12_mths_ex_med' not found in X_test\n",
      "KeyError: \"['collections_12_mths_ex_med'] not found in axis\" - One or more columns not found in DataFrame\n",
      "Warning: Column 'collections_12_mths_ex_med' not found in X_train\n",
      "Warning: Column 'collections_12_mths_ex_med' not found in X_val\n",
      "Warning: Column 'collections_12_mths_ex_med' not found in X_test\n",
      "KeyError: \"['collections_12_mths_ex_med'] not found in axis\" - One or more columns not found in DataFrame\n"
     ]
    }
   ],
   "source": [
    "# Add KeyError exception handling\n",
    "for col in drop_cols:\n",
    "    if col not in X_train.columns:\n",
    "        print(f\"Warning: Column '{col}' not found in X_train\")\n",
    "    if col not in X_val.columns:\n",
    "        print(f\"Warning: Column '{col}' not found in X_val\")\n",
    "    if col not in X_test.columns:\n",
    "        print(f\"Warning: Column '{col}' not found in X_test\")\n",
    "    try:\n",
    "        X_train.drop(columns=[col], inplace=True)\n",
    "        X_val.drop(columns=[col], inplace=True)\n",
    "        X_test.drop(columns=[col], inplace=True)\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e} - One or more columns not found in DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "698101dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>emp_title</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>Notes</th>\n",
       "      <th>purpose_cat</th>\n",
       "      <th>purpose</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>addr_state</th>\n",
       "      <th>debt_to_income</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>earliest_cr_line</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>mths_since_last_delinq</th>\n",
       "      <th>mths_since_last_record</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>mths_since_last_major_derog</th>\n",
       "      <th>policy_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4546</th>\n",
       "      <td>4547</td>\n",
       "      <td>U.S. Patent and Trademark Office</td>\n",
       "      <td>2</td>\n",
       "      <td>RENT</td>\n",
       "      <td>49861.0</td>\n",
       "      <td>VERIFIED - income</td>\n",
       "      <td>Borrower added on 05/05/10 &gt; My goal is to be ...</td>\n",
       "      <td>debt consolidation</td>\n",
       "      <td>Financial freedom</td>\n",
       "      <td>207xx</td>\n",
       "      <td>MD</td>\n",
       "      <td>16.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8/1/99</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13236</td>\n",
       "      <td>71.2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>PC4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4772</th>\n",
       "      <td>4773</td>\n",
       "      <td>Hopkins Distribution co.</td>\n",
       "      <td>1</td>\n",
       "      <td>RENT</td>\n",
       "      <td>24996.0</td>\n",
       "      <td>not verified</td>\n",
       "      <td>I wish to get a loan to pay off 2 Credet cards...</td>\n",
       "      <td>debt consolidation</td>\n",
       "      <td>Debt consolidation</td>\n",
       "      <td>895xx</td>\n",
       "      <td>NV</td>\n",
       "      <td>11.38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8/1/03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6291</td>\n",
       "      <td>24.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3</td>\n",
       "      <td>PC2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5650</th>\n",
       "      <td>5651</td>\n",
       "      <td>Moss Adams LLP</td>\n",
       "      <td>5</td>\n",
       "      <td>RENT</td>\n",
       "      <td>95000.0</td>\n",
       "      <td>VERIFIED - income source</td>\n",
       "      <td>Borrower added on 03/07/11 &gt; I plan on getti...</td>\n",
       "      <td>debt consolidation</td>\n",
       "      <td>Loan</td>\n",
       "      <td>900xx</td>\n",
       "      <td>CA</td>\n",
       "      <td>7.41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1/1/96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10954</td>\n",
       "      <td>35.1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2</td>\n",
       "      <td>PC1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3787</th>\n",
       "      <td>3788</td>\n",
       "      <td>The Aspen Group</td>\n",
       "      <td>4</td>\n",
       "      <td>RENT</td>\n",
       "      <td>23000.0</td>\n",
       "      <td>not verified</td>\n",
       "      <td>Borrower added on 07/19/11 &gt; I am relocating f...</td>\n",
       "      <td>moving</td>\n",
       "      <td>Moving</td>\n",
       "      <td>137xx</td>\n",
       "      <td>NY</td>\n",
       "      <td>21.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6/1/03</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1614</td>\n",
       "      <td>14.8</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3</td>\n",
       "      <td>PC4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7562</th>\n",
       "      <td>7563</td>\n",
       "      <td>IFC</td>\n",
       "      <td>2</td>\n",
       "      <td>OWN</td>\n",
       "      <td>182496.0</td>\n",
       "      <td>VERIFIED - income source</td>\n",
       "      <td>Hi all, this is a re-list - still ironing out ...</td>\n",
       "      <td>major purchase</td>\n",
       "      <td>Vineyard Property in NZ - Perfect Timing</td>\n",
       "      <td>117xx</td>\n",
       "      <td>NY</td>\n",
       "      <td>4.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3/1/92</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3</td>\n",
       "      <td>PC2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id                         emp_title emp_length home_ownership  annual_inc       verification_status  \\\n",
       "4546  4547  U.S. Patent and Trademark Office          2           RENT     49861.0         VERIFIED - income   \n",
       "4772  4773          Hopkins Distribution co.          1           RENT     24996.0              not verified   \n",
       "5650  5651                    Moss Adams LLP          5           RENT     95000.0  VERIFIED - income source   \n",
       "3787  3788                   The Aspen Group          4           RENT     23000.0              not verified   \n",
       "7562  7563                               IFC          2            OWN    182496.0  VERIFIED - income source   \n",
       "\n",
       "                                                  Notes         purpose_cat                                   purpose zip_code addr_state  debt_to_income  \\\n",
       "4546  Borrower added on 05/05/10 > My goal is to be ...  debt consolidation                         Financial freedom    207xx         MD           16.00   \n",
       "4772  I wish to get a loan to pay off 2 Credet cards...  debt consolidation                        Debt consolidation    895xx         NV           11.38   \n",
       "5650    Borrower added on 03/07/11 > I plan on getti...  debt consolidation                                      Loan    900xx         CA            7.41   \n",
       "3787  Borrower added on 07/19/11 > I am relocating f...              moving                                    Moving    137xx         NY           21.97   \n",
       "7562  Hi all, this is a re-list - still ironing out ...      major purchase  Vineyard Property in NZ - Perfect Timing    117xx         NY            4.73   \n",
       "\n",
       "      delinq_2yrs earliest_cr_line  inq_last_6mths  mths_since_last_delinq  mths_since_last_record  open_acc  pub_rec  revol_bal  revol_util  total_acc  \\\n",
       "4546          0.0           8/1/99             2.0                     NaN                     NaN       5.0      0.0      13236        71.2       12.0   \n",
       "4772          0.0           8/1/03             0.0                     NaN                     NaN      10.0      0.0       6291        24.0       16.0   \n",
       "5650          0.0           1/1/96             0.0                    61.0                     NaN       7.0      0.0      10954        35.1       17.0   \n",
       "3787          0.0           6/1/03             2.0                     NaN                     NaN       9.0      0.0       1614        14.8       20.0   \n",
       "7562          0.0           3/1/92             2.0                     NaN                     NaN       5.0      0.0          0         0.0       30.0   \n",
       "\n",
       "      mths_since_last_major_derog policy_code  \n",
       "4546                            1         PC4  \n",
       "4772                            3         PC2  \n",
       "5650                            2         PC1  \n",
       "3787                            3         PC4  \n",
       "7562                            3         PC2  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d210531e",
   "metadata": {},
   "source": [
    "# Consider dropping rows with missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddee1d91",
   "metadata": {},
   "source": [
    "# Imputing missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7081376",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
